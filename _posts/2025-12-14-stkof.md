---
title: STKOF
date: 2025-12-14 17:00:00 +0000
categories: [CTF, Upsolve]
tags: [pwnable]
math: true
mermaid: true
media_subpath: /assets/posts/2025-12-14-Stkof
image:
  path: preview.png
---

## STKOF

### Overview

**STKOF** is a classic heap exploitation challenge from *HITCON CTF 2014*, targeting *glibc 2.23*. The binary contains a heap-based overflow vulnerability that allows corruption of adjacent chunk metadata. By abusing this corruption, we can perform the unlink attack, leading to arbitrary read/write.

Using this primitive, we leak the libc address and overwrite the GOT entry of *free* with *system*, ultimately achieving code execution and spawning a shell.

Challenge files are available [here](https://github.com/guyinatuxedo/nightmare/tree/master/modules/30-unlink/hitcon14_stkof).

### Program Analysis

We are provided with the executable *stkof* and the corresponding libc file *libc-2.23.so*.

Since the binary is dynamically linked and uses our system libc by default, we need to patch it to use the provided libc. This can be done using pwninit:

```bash
pwninit --bin stkof --libc libc-2.23.so --no-template
```

Checking the file type and enabled protections, we get the following:

```bash
~/Desktop/BinExp/Challs/HEAP/Stkof ❯ file stkof
stkof: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, for GNU/Linux 2.6.32, BuildID[sha1]=4872b087443d1e52ce720d0a4007b1920f18e7b0, stripped
                                                                                                                                                                                             
~/Desktop/BinExp/Challs/HEAP/Stkof ❯ checksec stkof
[*] '/home/mark/Desktop/BinExp/Challs/HEAP/Stkof/stkof'
    Arch:       amd64-64-little
    RELRO:      Partial RELRO
    Stack:      Canary found
    NX:         NX enabled
    PIE:        No PIE (0x400000)
```

Shows the standard you'd expect... except that it has *Partial RELRO* enabled meaning the *Global Offset Table (GOT)* is writable and *Position Independent Executable (PIE)* is disabled.

Running the patched binary to get a high-level idea of its behavior:

```bash
~/Desktop/BinExp/Challs/HEAP/Stkof ❯ ./stkof_patched
asdf
FAIL
help
FAIL
haha
FAIL
exit
FAIL
1
^C

```

Notice how it keeps printing *FAIL* regardless of the data we provide.

### Reversing

In order to understand this binary logic we need to reverse engineer it. Note that the binary is stripped, so any decompilation shown later is based on manually reconstructed pseudocode and renamed functions.

Here's the *main* function:

```c
__int64 __fastcall main(int a1, char **a2, char **a3)
{
  int choice; // eax
  int v1; // [rsp+Ch] [rbp-74h]
  char nptr[104]; // [rsp+10h] [rbp-70h] BYREF
  unsigned __int64 v7; // [rsp+78h] [rbp-8h]

  v7 = __readfsqword(0x28u);
  alarm(120u);
  while ( fgets(nptr, 10, stdin) )
  {
    choice = atoi(nptr);
    if ( choice == 2 )
    {
      v1 = edit_chunk();
      goto err;
    }
    if ( choice > 2 )
    {
      if ( choice == 3 )
      {
        v1 = delete_chunk();
        goto err;
      }
      if ( choice == 4 )
      {
        v1 = write_chunk();
        goto err;
      }
    }
    else if ( choice == 1 )
    {
      v1 = allocate_chunk();
      goto err;
    }
    v1 = -1;
err:
    if ( v1 )
      puts("FAIL");
    else
      puts("OK");
    fflush(stdout);
  }
  return 0LL;
}
```

It simply sets up a 120-second timer and then enters a while loop where it reads the user’s choice and calls the corresponding function.

*allocate_chunk* function:

```c
__int64 allocate_chunk()
{
  __int64 size; // [rsp+0h] [rbp-80h]
  char *ptr; // [rsp+8h] [rbp-78h]
  char s[104]; // [rsp+10h] [rbp-70h] BYREF
  unsigned __int64 v4; // [rsp+78h] [rbp-8h]

  v4 = __readfsqword(0x28u);
  fgets(s, 16, stdin);
  size = atoll(s);
  ptr = (char *)malloc(size);
  if ( !ptr )
    return 0xFFFFFFFFLL;
  chunk[++idx] = ptr;
  printf("%d\n", idx);
  return 0LL;
}
```

- The function allocates heap memory using a user-controlled size.
- If malloc fails, the function returns -1.
- On success, the returned pointer is stored in the global chunk array.
- The index *idx* is incremented before use, meaning chunk indexing starts at 1, not 0.

*edit_chunk* function:

```c
__int64 edit_chunk()
{
  int i; // eax
  unsigned int idx; // [rsp+8h] [rbp-88h]
  __int64 size; // [rsp+10h] [rbp-80h]
  char *ptr; // [rsp+18h] [rbp-78h]
  char buf[104]; // [rsp+20h] [rbp-70h] BYREF
  unsigned __int64 v6; // [rsp+88h] [rbp-8h]

  v6 = __readfsqword(0x28u);
  fgets(buf, 16, stdin);
  idx = atol(buf);
  if ( idx > 0x100000 )
    return 0xFFFFFFFFLL;
  if ( !chunk[idx] )
    return 0xFFFFFFFFLL;
  fgets(buf, 16, stdin);
  size = atoll(buf);
  ptr = chunk[idx];
  for ( i = fread(ptr, 1uLL, size, stdin); i > 0; i = fread(ptr, 1uLL, size, stdin) )
  {
    ptr += i;
    size -= i;
  }
  if ( size )
    return 0xFFFFFFFFLL;
  else
    return 0LL;
}
```

- The function takes a *chunk index* and a *user-controlled* size.
- It ensures that *idx* is within the bounds of the *chunk* array.
- Data is written directly into `chunk[idx]` using *fread* (if it exists..), which allows *arbitrary-length* writes as we control the *size*.
- Because *ptr* is incremented while *size* is decremented, the function continues writing until all requested bytes are consumed.

*delete_chunk* function:

```c
__int64 delete_chunk()
{
  unsigned int idx; // [rsp+Ch] [rbp-74h]
  char buf[104]; // [rsp+10h] [rbp-70h] BYREF
  unsigned __int64 v3; // [rsp+78h] [rbp-8h]

  v3 = __readfsqword(0x28u);
  fgets(buf, 16, stdin);
  idx = atol(buf);
  if ( idx > 0x100000 )
    return 0xFFFFFFFFLL;
  if ( !chunk[idx] )
    return 0xFFFFFFFFLL;
  free(chunk[idx]);
  chunk[idx] = 0LL;
  return 0LL;
}
```

- The function takes a *chunk index*.
- It ensures that *idx* is within the bounds of the *chunk* array.
- And *frees* the memory stored in *chunk* at index *idx*.
- It also clears the pointer stored there.

*write_chunk* function:

```c
__int64 write_chunk()
{
  unsigned int idx; // [rsp+Ch] [rbp-74h]
  char s[104]; // [rsp+10h] [rbp-70h] BYREF
  unsigned __int64 v3; // [rsp+78h] [rbp-8h]

  v3 = __readfsqword(0x28u);
  fgets(s, 16, stdin);
  idx = atol(s);
  if ( idx > 0x100000 )
    return 0xFFFFFFFFLL;
  if ( !chunk[idx] )
    return 0xFFFFFFFFLL;
  if ( strlen(chunk[idx]) <= 3 )
    puts("//TODO");
  else
    puts("...");
  return 0LL;
}
```

- The function takes a chunk index and verifies that it is within bounds and points to a valid entry in the chunk array.
- It then checks the length of the data stored at `chunk[idx]` using strlen.
- If the length is less than or equal to 3, it prints "//TODO"; otherwise, it prints "...".

Despite its name, this function does not print the contents of the chunk. Instead, it only inspects the length of the stored data and prints a placeholder message.

### Exploitation

#### So, what's the vulnerability?

The vulnerability is a *heap-based buffer overflow* in the *edit_chunk* function. The function uses a user‑controlled size and writes up to n bytes into the heap without verifying that this size matches the originally allocated chunk.

As a result, we can overflow one chunk and corrupt the metadata of adjacent heap chunks.

<div align="center">
  <img src="spongebob-fry-cook.gif" alt="cook" width="400">
  <br>
  <em>Shall we?</em>
</div>


#### Leaks Plan

To exploit this binary, we need to find a primitive that gives us an arbitrary `read/write`.

Since this challenge targets an older version of glibc (2.23), it does not include the `Thread Local Caching (tcache) Bin`, meaning allocations and frees follow the classic bin behavior (fastbin / unsorted bin / small bin / largebin).

Looking at the *write_chunk* function, we see a potential target for getting a libc leak (supposing we have *arb write*).

```c
  if ( strlen(chunk[idx]) <= 3 )
    puts("//TODO");
```

Basically if we can control what's stored in *chunk[idx]* and we can overwrite *strlen* then this means we can get a libc leak, as we can easily set the *GOT* of *strlen* to *puts@plt* and *chunk[idx]* to *puts@got*, essentially ending up being this:

```c
puts(puts)
```

This works well because there are no other references to *strlen*, so overwriting its GOT entry is safe.

But how do we actually get an *arbitrary read/write primitive*? This is where we need to leverage the allocator to our advantage.

We could easily corrupt a fastbin (it’s a singly linked list), but with all the checks and requirements involved... uhhh, let’s do something better.

<div align="center">
  <img src="nah.gif" alt="cook" width="400">
  <br>
  <em>Fastbins? Nah, we’re cooking something better</em>
</div>


#### Heaps Of Fun

Note: This really isn't a detailed explanation on how the heap works...

When a program requests dynamic memory, the allocator extends the process’s data segment using the sbrk syscall, creating a region known as the heap. This region is managed internally by glibc using heap chunks and linked lists.

- Arena: A structure that is shared among one or more threads which contains references to one or more heaps, as well as linked lists of chunks within those heaps which are "free". Threads assigned to each arena will allocate memory from that arena's free lists. 
- Heap: A contiguous region of memory that is subdivided into chunks to be allocated. Each heap belongs to exactly one arena. 
- Chunk: A small range of memory that can be allocated (owned by the application), freed (owned by glibc), or combined with adjacent chunks into larger ranges. Note that a chunk is a wrapper around the block of memory that is given to the application. Each chunk exists in one heap and belongs to one arena. 
- Memory: A portion of the application's address space which is typically backed by RAM or swap. 

Glibc's malloc is chunk-oriented. It divides a large region of memory (a "heap") into chunks of various sizes. Each chunk includes meta-data about how big it is (via a size field in the chunk header), and thus where the adjacent chunks are.

When a chunk is in use by the application, the only data that's "remembered" is the size of the chunk. When the chunk is free'd, the memory that used to be application data is re-purposed for additional arena-related information, such as pointers within linked lists, such that suitable chunks can quickly be found and re-used when needed. 

Also, the last word in a free'd chunk contains a copy of the chunk size (with the three LSBs set to zeros, vs the three LSBs of the size at the front of the chunk which are used for flags). 

This is the structure of a chunk allocated by `malloc`.

```c
struct malloc_chunk {

  INTERNAL_SIZE_T      prev_size;  /* Size of previous chunk (if free).  */
  INTERNAL_SIZE_T      size;       /* Size in bytes, including overhead. */

  struct malloc_chunk* fd;         /* double links -- used only if free. */
  struct malloc_chunk* bk;

  /* Only used for large blocks: pointer to next larger size.  */
  struct malloc_chunk* fd_nextsize; /* double links -- used only if free. */
  struct malloc_chunk* bk_nextsize;
};
```

An allocated chunk looks like this:

```
    chunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
	    |             Size of previous chunk, if allocated            | |
	    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
	    |             Size of chunk, in bytes                        M|P|
      mem-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
	    |             User data starts here...                          .
	    .                                                               .
	    .             (malloc_usable_size() bytes)                      .
	    .                                                               |
nextchunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
	    |             Size of chunk                                     |
	    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
```

Where "chunk" is the start of the chunk for the purpose of most of the malloc code, but "mem" is the pointer that is returned to the user.  

"Nextchunk" is the beginning of the next contiguous chunk.

This is how it looks like in memory when we allocate a chunk of `0x20` bytes.

```
0x603000|+0x00000|+0x00000: 0x0000000000000000 0x0000000000000031 | ........1....... |
0x603010|+0x00010|+0x00010: 0x0000004141414141 0x0000000000000000 | AAAAA........... |
0x603020|+0x00020|+0x00020: 0x0000000000000000 0x0000000000000000 | ................ |
0x603030|+0x00000|+0x00030: 0x0000000000000000 0x0000000000020fd1 | ................ |  <-  top
```

The bit flags of the size field holds:
- Allocated Arena (A): The main arena uses the application's heap. Other arenas use mmap'd heaps. To map a chunk to a heap, you need to know which case applies. If this bit is 0, the chunk comes from the main arena and the main heap. If this bit is 1, the chunk comes from mmap'd memory and the location of the heap can be computed from the chunk's address. 
- MMap'd chunk (M): this chunk was allocated with a single call to *mmap* and is not part of a heap at all. 
- Previous chunk is in use (P): if set, the previous chunk is still being used by the application, and thus the prev_size field is invalid.

```c
/* size field is or'ed with PREV_INUSE when previous adjacent chunk in use */
#define PREV_INUSE 0x1

/* extract inuse bit of previous chunk */
#define prev_inuse(p)       ((p)->size & PREV_INUSE)

/* size field is or'ed with IS_MMAPPED if the chunk was obtained with mmap() */
#define IS_MMAPPED 0x2

/* check for mmap()'ed chunk */
#define chunk_is_mmapped(p) ((p)->size & IS_MMAPPED)

/* size field is or'ed with NON_MAIN_ARENA if the chunk was obtained
   from a non-main arena.  This is only set immediately before handing
   the chunk to the user, if necessary.  */
#define NON_MAIN_ARENA 0x4

/* check for chunk from non-main arena */
#define chunk_non_main_arena(p) ((p)->size & NON_MAIN_ARENA)
```

If you want to read more on malloc internals check this out [reference](https://sourceware.org/glibc/wiki/MallocInternals)

Here we will focus on what happens when we free a chunk.

There's no function in the glibc source code called *free*, it's only an alias for the function *__libc_free*.

```c
strong_alias (__libc_free, __free) strong_alias (__libc_free, free)
```

When we call *free*, we pass the pointer which needs to be freed as the first parameter.

In glibc, the free function first checks the *__free_hook* pointer:
- __free_hook is a function pointer that glibc calls whenever free is invoked, if it is set.
- The hook is read atomically, and if it is non-null, the function pointer is called with the memory being freed as an argument.
- After the hook is executed the function returns.
- In exploitation, this hook (present in glibc < 2.34) is commonly used to gain code by overwriting *__free_hook* with system and freeing a chunk which has */bin/sh* as the first quadword.

```c
void
__libc_free (void *mem)
{
  mstate ar_ptr;
  mchunkptr p;                          /* chunk corresponding to mem */

  void (*hook) (void *, const void *)
    = atomic_forced_read (__free_hook);
  if (__builtin_expect (hook != NULL, 0))
    {
      (*hook)(mem, RETURN_ADDRESS (0));
      return;
    }

  if (mem == 0)                              /* free(0) has no effect */
    return;
```

If *__free_hook* is null:
- It converts our mem to a chunk, this is achieved by subtracting it with 0x10, see macro below.
- It then checks if the chunk was allocated via a call to *mmap*, and if it is, it unmaps it from memory.
- If that isn't the case, it gets the current arena pointer (which in this case would be the main arena as the program has 1 thread which is the main thread) and calls the *_int_free* function.

```c

#define chunk2mem(p)   ((void*)((char*)(p) + 2*SIZE_SZ))
#define mem2chunk(mem) ((mchunkptr)((char*)(mem) - 2*SIZE_SZ))

  p = mem2chunk (mem);

  if (chunk_is_mmapped (p))                       /* release mmapped memory. */
    {
      /* see if the dynamic brk/mmap threshold needs adjusting */
      if (!mp_.no_dyn_threshold
          && p->size > mp_.mmap_threshold
          && p->size <= DEFAULT_MMAP_THRESHOLD_MAX)
        {
          mp_.mmap_threshold = chunksize (p);
          mp_.trim_threshold = 2 * mp_.mmap_threshold;
          LIBC_PROBE (memory_mallopt_free_dyn_thresholds, 2,
                      mp_.mmap_threshold, mp_.trim_threshold);
        }
      munmap_chunk (p);
      return;
    }

  ar_ptr = arena_for_chunk (p);
  _int_free (ar_ptr, p, 0);

```

The *_int_free* function is responsible for handling the core logic of freed chunks.

Free chunks are stored in various lists based on size and history, so that the library can quickly find suitable chunks to satisfy allocation requests. The lists, called "bins", are: 

- Fast: Small chunks are stored in size-specific bins. Chunks added to a fast bin ("fastbin") are not combined with adjacent chunks - the logic is minimal to keep access fast (hence the name). Chunks in the fastbins may be moved to other bins as needed. Fastbin chunks are stored in an array of singly-linked lists, since they're all the same size and chunks in the middle of the list need never be accessed. 

- Unsorted bin: When chunks are free'd they're initially stored in a single bin. They're sorted later, in malloc, in order to give them one chance to be quickly re-used. This also means that the sorting logic only needs to exist at one point - everyone else just puts free'd chunks into this bin, and they'll get sorted later. The "unsorted" bin is simply the first of the regular bins. 

- Small: The normal bins are divided into "small" bins, where each chunk is the same size, and "large" bins, where chunks are a range of sizes. When a chunk is added to these bins, they're first combined with adjacent chunks to "coalesce" them into larger chunks. Thus, these chunks are never adjacent to other such chunks (although they may be adjacent to fast or unsorted chunks, and of course in-use chunks). Small and large chunks are doubly-linked so that chunks may be removed from the middle (such as when they're combined with newly free'd chunks). 

- Large: A chunk is "large" if its bin may contain more than one size. For small bins, you can pick the first chunk and just use it. For large bins, you have to find the "best" chunk, and possibly split it into two chunks (one the size you need, and one for the remainder). 

<div align="center">
  <img src="arena.png" alt="arena" width="400">
  <br>
  <em>malloc_state (arena)</em>
</div>

```c
/* Forward declarations.  */
struct malloc_chunk;
typedef struct malloc_chunk* mchunkptr;

struct malloc_state
{
  /* Serialize access.  */
  mutex_t mutex;

  /* Flags (formerly in max_fast).  */
  int flags;

  /* Fastbins */
  mfastbinptr fastbinsY[NFASTBINS];

  /* Base of the topmost chunk -- not otherwise kept in a bin */
  mchunkptr top;

  /* The remainder from the most recent split of a small request */
  mchunkptr last_remainder;

  /* Normal bins packed as described above */
  mchunkptr bins[NBINS * 2 - 2];

  /* Bitmap of bins */
  unsigned int binmap[BINMAPSIZE];

  /* Linked list */
  struct malloc_state *next;

  /* Linked list for free arenas.  Access to this field is serialized
     by free_list_lock in arena.c.  */
  struct malloc_state *next_free;

  /* Number of threads attached to this arena.  0 if the arena is on
     the free list.  Access to this field is serialized by
     free_list_lock in arena.c.  */
  INTERNAL_SIZE_T attached_threads;

  /* Memory allocated from the system in this arena.  */
  INTERNAL_SIZE_T system_mem;
  INTERNAL_SIZE_T max_system_mem;
};

typedef struct malloc_state *mstate;

static struct malloc_state main_arena =
{
  .mutex = _LIBC_LOCK_INITIALIZER,
  .next = &main_arena,
  .attached_threads = 1
};

```

#### References
- https://sourceware.org/glibc/wiki/MallocInternals
- https://elixir.bootlin.com/glibc/glibc-2.23/source/malloc/malloc.c
- https://4xura.com/binex/glibc/ptmalloc2/ptmalloc-the-gnu-allocator-a-deep-gothrough-on-how-malloc-free-works/
- https://github.com/shellphish/how2heap/blob/master/glibc_2.23/unsafe_unlink.c